{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RL Text-to-SQL Training on Google Colab\n\nThis notebook demonstrates how to train a Text-to-SQL model with RL on a 24GB GPU (Colab A100).\n\n**Runtime:** GPU (A100 recommended)\n\n**Estimated time:** 2-4 hours for 3 epochs on Spider train set","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup","metadata":{}},{"cell_type":"code","source":"# Check GPU\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:07:43.498931Z","iopub.execute_input":"2026-02-12T19:07:43.499463Z","iopub.status.idle":"2026-02-12T19:07:43.981071Z","shell.execute_reply.started":"2026-02-12T19:07:43.499428Z","shell.execute_reply":"2026-02-12T19:07:43.980275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clone repository\n!git clone https://github.com/GameChaser782/rl-text2sql.git\n%cd rl-text2sql","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:16:26.336088Z","iopub.execute_input":"2026-02-12T19:16:26.336786Z","iopub.status.idle":"2026-02-12T19:16:26.736843Z","shell.execute_reply.started":"2026-02-12T19:16:26.336752Z","shell.execute_reply":"2026-02-12T19:16:26.736119Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install dependencies\n!pip install -q -U pip\n!pip install -q transformers>=4.35.0 accelerate>=0.25.0 peft>=0.7.0 bitsandbytes>=0.41.0\n!pip install -q trl>=0.7.0 datasets>=2.14.0 pyyaml timeout-decorator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:16:33.561861Z","iopub.execute_input":"2026-02-12T19:16:33.562605Z","iopub.status.idle":"2026-02-12T19:16:50.155977Z","shell.execute_reply.started":"2026-02-12T19:16:33.562572Z","shell.execute_reply":"2026-02-12T19:16:50.155237Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Download Spider Dataset","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport sqlite3\n\n# Create minimal test data\nos.makedirs('data/spider/database/test_db', exist_ok=True)\n\n# Create a simple test database\nconn = sqlite3.connect('data/spider/database/test_db/test_db.sqlite')\ncursor = conn.cursor()\ncursor.execute('CREATE TABLE students (id INTEGER, name TEXT, age INTEGER)')\ncursor.execute('INSERT INTO students VALUES (1, \"Alice\", 20), (2, \"Bob\", 22)')\nconn.commit()\nconn.close()\n\n# Create test JSON data\ntest_data = [\n    {\n        \"question\": \"What are the names of all students?\",\n        \"query\": \"SELECT name FROM students\",\n        \"db_id\": \"test_db\"\n    },\n    {\n        \"question\": \"How many students are there?\",\n        \"query\": \"SELECT COUNT(*) FROM students\", \n        \"db_id\": \"test_db\"\n    }\n]\n\nwith open('data/spider/train_spider.json', 'w') as f:\n    json.dump(test_data, f)\n    \nwith open('data/spider/dev.json', 'w') as f:\n    json.dump(test_data[:1], f)\n\nprint(\"✅ Created synthetic test data\")\n!ls -la data/spider/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:21:01.732366Z","iopub.execute_input":"2026-02-12T19:21:01.732709Z","iopub.status.idle":"2026-02-12T19:21:01.866785Z","shell.execute_reply.started":"2026-02-12T19:21:01.732680Z","shell.execute_reply":"2026-02-12T19:21:01.866113Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Quick Test: Reward Function","metadata":{}},{"cell_type":"code","source":"from reward import SQLRewardCalculator, RewardConfig\n\n# Initialize reward calculator\nconfig = RewardConfig(\n    execution_weight=1.0,\n    partial_weight=0.3,\n    use_partial_rewards=True\n)\n\nreward_calc = SQLRewardCalculator(db_path=\"dummy.db\", config=config)\n\n# Test partial rewards\npred_sql = \"SELECT name, age FROM users WHERE age > 18\"\ngold_sql = \"SELECT name, age FROM users WHERE age > 18 ORDER BY age\"\n\npartial = reward_calc.partial_rewards(pred_sql, gold_sql)\nprint(f\"Partial reward: {partial:.3f}\")\n\n# Test component extraction\ncomponents = reward_calc._extract_sql_components(pred_sql)\nprint(f\"\\nSQL Components:\")\nfor comp_type, comp_set in components.items():\n    print(f\"  {comp_type}: {comp_set}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:21:12.479508Z","iopub.execute_input":"2026-02-12T19:21:12.480311Z","iopub.status.idle":"2026-02-12T19:21:12.497083Z","shell.execute_reply.started":"2026-02-12T19:21:12.480277Z","shell.execute_reply":"2026-02-12T19:21:12.496519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Load Model and Test Generation","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# Model name - choose smaller model for faster testing\nmodel_name = \"Qwen/Qwen2.5-Coder-3B-Instruct\"  # 3B parameters\n# Alternative: \"codellama/CodeLlama-7b-hf\"\n\nprint(f\"Loading {model_name}...\")\n\n# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\n# Prepare for training\nmodel = prepare_model_for_kbit_training(model)\n\n# LoRA config\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Add LoRA\nmodel = get_peft_model(model, lora_config)\nmodel.gradient_checkpointing_enable()\n\nmodel.print_trainable_parameters()\n\nprint(\"\\nModel loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:21:19.372275Z","iopub.execute_input":"2026-02-12T19:21:19.372575Z","iopub.status.idle":"2026-02-12T19:22:41.449508Z","shell.execute_reply.started":"2026-02-12T19:21:19.372551Z","shell.execute_reply":"2026-02-12T19:22:41.448905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test generation\ndef generate_sql(question, model, tokenizer):\n    prompt = f\"Question: {question}\\nSQL:\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=128,\n        temperature=0.7,\n        do_sample=True\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response\n\n# Test\nquestion = \"What are the names of all students?\"\nresponse = generate_sql(question, model, tokenizer)\nprint(f\"Question: {question}\")\nprint(f\"Response: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:22:45.100183Z","iopub.execute_input":"2026-02-12T19:22:45.101537Z","iopub.status.idle":"2026-02-12T19:22:56.345161Z","shell.execute_reply.started":"2026-02-12T19:22:45.101505Z","shell.execute_reply":"2026-02-12T19:22:56.344401Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Prepare Training Data (Small Subset)","metadata":{}},{"cell_type":"code","source":"import json\n\n# Load Spider data\nwith open('data/spider/train_spider.json', 'r') as f:\n    spider_data = json.load(f)\n\nprint(f\"Total training examples: {len(spider_data)}\")\n\n# For quick testing, use a small subset\nsubset_size = 100  # Adjust based on time constraints\ntrain_subset = spider_data[:subset_size]\n\n# Save subset\nwith open('data/spider/train_subset.json', 'w') as f:\n    json.dump(train_subset, f)\n\nprint(f\"Using {len(train_subset)} examples for training\")\nprint(f\"\\nExample:\")\nprint(json.dumps(train_subset[0], indent=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:23:26.100815Z","iopub.execute_input":"2026-02-12T19:23:26.101099Z","iopub.status.idle":"2026-02-12T19:23:26.107804Z","shell.execute_reply.started":"2026-02-12T19:23:26.101075Z","shell.execute_reply":"2026-02-12T19:23:26.107183Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Training Configuration","metadata":{}},{"cell_type":"code","source":"# Create config for quick training\nconfig = {\n    'model_name': model_name,\n    'use_qlora': True,\n    'train_data': 'data/spider/train_subset.json',\n    'db_root': 'data/spider/database',\n    'num_samples': 4,\n    'temperature': 0.7,\n    'num_epochs': 2,  # Reduced for quick testing\n    'batch_size': 1,\n    'gradient_accumulation_steps': 4,  # Reduced for speed\n    'learning_rate': 1e-5,\n    'kl_coef': 0.1,\n    'execution_weight': 1.0,\n    'partial_weight': 0.3,\n    'output_dir': 'outputs/rl-model',\n    'seed': 42\n}\n\n# Save config\nimport yaml\nwith open('config_colab.yaml', 'w') as f:\n    yaml.dump(config, f)\n\nprint(\"Configuration:\")\nprint(yaml.dump(config, default_flow_style=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:23:32.226349Z","iopub.execute_input":"2026-02-12T19:23:32.226674Z","iopub.status.idle":"2026-02-12T19:23:32.234720Z","shell.execute_reply.started":"2026-02-12T19:23:32.226648Z","shell.execute_reply":"2026-02-12T19:23:32.233947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n# Verify only one GPU visible\nimport torch\nprint(f\"GPUs available: {torch.cuda.device_count()}\")\nprint(f\"Using: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:34:02.984981Z","iopub.execute_input":"2026-02-12T19:34:02.985470Z","iopub.status.idle":"2026-02-12T19:34:02.990615Z","shell.execute_reply.started":"2026-02-12T19:34:02.985435Z","shell.execute_reply":"2026-02-12T19:34:02.989944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check what's in the config\n!cat config_colab.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:36:05.904063Z","iopub.execute_input":"2026-02-12T19:36:05.904826Z","iopub.status.idle":"2026-02-12T19:36:06.081759Z","shell.execute_reply.started":"2026-02-12T19:36:05.904792Z","shell.execute_reply":"2026-02-12T19:36:06.080736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read train_rl.py\nwith open('train_rl.py', 'r') as f:\n    content = f.read()\n\n# Remove the default model\ncontent = content.replace(\n    \"parser.add_argument('--model_name', type=str, default='codellama/CodeLlama-7b-hf',\",\n    \"parser.add_argument('--model_name', type=str, default=None,\"\n)\n\n# Write back\nwith open('train_rl.py', 'w') as f:\n    f.write(content)\n\nprint(\"✅ Fixed train_rl.py - removed default model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:37:42.292800Z","iopub.execute_input":"2026-02-12T19:37:42.293675Z","iopub.status.idle":"2026-02-12T19:37:42.299395Z","shell.execute_reply.started":"2026-02-12T19:37:42.293639Z","shell.execute_reply":"2026-02-12T19:37:42.298743Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Run Training","metadata":{}},{"cell_type":"code","source":"# Run with both config AND required args (bug in script)\n!python train_rl.py \\\n    --config config_colab.yaml \\\n    --train_data data/spider/train_subset.json \\\n    --db_root data/spider/database","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:42:35.579918Z","iopub.execute_input":"2026-02-12T19:42:35.580264Z","iopub.status.idle":"2026-02-12T19:54:43.758436Z","shell.execute_reply.started":"2026-02-12T19:42:35.580230Z","shell.execute_reply":"2026-02-12T19:54:43.757487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# In another cell, monitor GPU usage\n!watch -n 1 nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T19:42:10.002977Z","iopub.execute_input":"2026-02-12T19:42:10.003270Z","iopub.status.idle":"2026-02-12T19:42:13.558089Z","shell.execute_reply.started":"2026-02-12T19:42:10.003213Z","shell.execute_reply":"2026-02-12T19:42:13.557385Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Evaluation","metadata":{}},{"cell_type":"code","source":"# Create a new working evaluate script\nevaluate_code = '''\nimport torch\nimport argparse\nimport json\nfrom pathlib import Path\nfrom typing import List, Dict\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\nfrom reward import SQLRewardCalculator, RewardConfig\n\nclass Text2SQLEvaluator:\n    def __init__(self, model, tokenizer, reward_calculator, device=\"cuda\"):\n        self.model = model.to(device)\n        self.model.eval()\n        self.tokenizer = tokenizer\n        self.reward_calculator = reward_calculator\n        self.device = device\n    \n    def create_prompt(self, question: str, schema: str = None) -> str:\n        if schema:\n            prompt = f\"\"\"Given the database schema:\n{schema}\n\nQuestion: {question}\n\nGenerate a SQL query to answer this question:\nSQL:\"\"\"\n        else:\n            prompt = f\"Question: {question}\\\\nSQL:\"\n        return prompt\n    \n    @torch.no_grad()\n    def generate_sql(self, question: str, schema: str = None) -> str:\n        prompt = self.create_prompt(question, schema)\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n        outputs = self.model.generate(**inputs, max_new_tokens=256, temperature=0.1, do_sample=False, pad_token_id=self.tokenizer.pad_token_id)\n        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        sql = self.extract_sql(generated_text)\n        return sql\n    \n    def extract_sql(self, text: str) -> str:\n        text = text.strip()\n        select_idx = text.upper().find('SELECT')\n        if select_idx != -1:\n            sql = text[select_idx:]\n            semicolon_idx = sql.find(';')\n            if semicolon_idx != -1:\n                sql = sql[:semicolon_idx]\n            return sql.strip()\n        return text\n    \n    def evaluate(self, test_data: List[Dict], db_root: str, output_file: str = None) -> Dict[str, float]:\n        total = len(test_data)\n        execution_correct = 0\n        exact_match = 0\n        predictions = []\n        \n        for example in tqdm(test_data, desc=\"Evaluating\"):\n            question = example['question']\n            gold_sql = example['query']\n            db_id = example['db_id']\n            db_path = f\"{db_root}/{db_id}/{db_id}.sqlite\"\n            schema = example.get('schema')\n            \n            pred_sql = self.generate_sql(question, schema)\n            \n            reward_dict = self.reward_calculator.compute_reward(pred_sql, gold_sql, question, db_path)\n            \n            if reward_dict['execution'] == 1.0:\n                execution_correct += 1\n            \n            if pred_sql.strip().upper() == gold_sql.strip().upper():\n                exact_match += 1\n            \n            predictions.append({\n                'question': question,\n                'gold_sql': gold_sql,\n                'pred_sql': pred_sql,\n                'db_id': db_id,\n                'execution_correct': reward_dict['execution'] == 1.0,\n                'exact_match': pred_sql.strip().upper() == gold_sql.strip().upper()\n            })\n        \n        metrics = {\n            'execution_accuracy': execution_correct / total if total > 0 else 0,\n            'exact_match': exact_match / total if total > 0 else 0,\n            'total': total\n        }\n        \n        if output_file:\n            with open(output_file, 'w') as f:\n                json.dump({'metrics': metrics, 'predictions': predictions}, f, indent=2)\n        \n        return metrics\n\ndef main(args):\n    print(\"=\"*80)\n    print(\"Text-to-SQL Evaluation\")\n    print(\"=\"*80)\n    \n    print(f\"\\\\nLoading test data from {args.test_data}...\")\n    with open(args.test_data, 'r') as f:\n        test_data = json.load(f)\n    print(f\"Test examples: {len(test_data)}\")\n    \n    print(f\"\\\\nLoading model from {args.model_path}...\")\n    \n    # Load tokenizer from base model\n    tokenizer = AutoTokenizer.from_pretrained(args.base_model)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Load base model\n    base = AutoModelForCausalLM.from_pretrained(args.base_model, device_map={\"\": 0}, torch_dtype=torch.bfloat16)\n    \n    # Load LoRA adapters\n    model = PeftModel.from_pretrained(base, args.model_path)\n    \n    reward_config = RewardConfig(execution_weight=1.0, partial_weight=0.0, timeout_seconds=5, use_partial_rewards=False)\n    reward_calculator = SQLRewardCalculator(db_path=\"\", config=reward_config)\n    \n    evaluator = Text2SQLEvaluator(model=model, tokenizer=tokenizer, reward_calculator=reward_calculator, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    print(\"\\\\nEvaluating...\")\n    metrics = evaluator.evaluate(test_data=test_data, db_root=args.db_root, output_file=args.output_file)\n    \n    print(\"\\\\n\" + \"=\"*80)\n    print(\"RESULTS\")\n    print(\"=\"*80)\n    print(f\"Execution Accuracy: {metrics['execution_accuracy']:.2%}\")\n    print(f\"Exact Match:        {metrics['exact_match']:.2%}\")\n    print(f\"Total Examples:     {metrics['total']}\")\n    print(\"=\"*80)\n    \n    if args.output_file:\n        print(f\"\\\\nPredictions saved to {args.output_file}\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Evaluate Text-to-SQL model\")\n    parser.add_argument('--model_path', type=str, required=True)\n    parser.add_argument('--base_model', type=str, required=True)\n    parser.add_argument('--test_data', type=str, required=True)\n    parser.add_argument('--db_root', type=str, required=True)\n    parser.add_argument('--output_file', type=str)\n    args = parser.parse_args()\n    main(args)\n'''\n\nwith open('evaluate_fixed.py', 'w') as f:\n    f.write(evaluate_code)\n\nprint(\"✅ Created evaluate_fixed.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T20:27:09.034376Z","iopub.execute_input":"2026-02-12T20:27:09.034966Z","iopub.status.idle":"2026-02-12T20:27:09.043866Z","shell.execute_reply.started":"2026-02-12T20:27:09.034932Z","shell.execute_reply":"2026-02-12T20:27:09.043121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -la outputs/rl-text2sql/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T20:30:35.392700Z","iopub.execute_input":"2026-02-12T20:30:35.393397Z","iopub.status.idle":"2026-02-12T20:30:35.568418Z","shell.execute_reply.started":"2026-02-12T20:30:35.393360Z","shell.execute_reply":"2026-02-12T20:30:35.567672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T20:32:13.919856Z","iopub.execute_input":"2026-02-12T20:32:13.920546Z","iopub.status.idle":"2026-02-12T20:32:14.096230Z","shell.execute_reply.started":"2026-02-12T20:32:13.920513Z","shell.execute_reply":"2026-02-12T20:32:14.094994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python evaluate_fixed.py \\\n    --model_path outputs/rl-text2sql \\\n    --base_model Qwen/Qwen2.5-Coder-3B-Instruct \\\n    --test_data data/spider/dev.json \\\n    --db_root data/spider/database \\\n    --output_file results/predictions.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T20:32:22.281057Z","iopub.execute_input":"2026-02-12T20:32:22.281679Z","iopub.status.idle":"2026-02-12T20:33:00.822753Z","shell.execute_reply.started":"2026-02-12T20:32:22.281647Z","shell.execute_reply":"2026-02-12T20:33:00.821792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and display results\nwith open('results/predictions.json', 'r') as f:\n    results = json.load(f)\n\nprint(\"=\" * 80)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\" * 80)\nprint(f\"Execution Accuracy: {results['metrics']['execution_accuracy']:.2%}\")\nprint(f\"Exact Match:        {results['metrics']['exact_match']:.2%}\")\nprint(f\"Total Examples:     {results['metrics']['total']}\")\nprint(\"=\" * 80)\n\n# Show some examples\nprint(\"\\nSample Predictions:\")\nfor i, pred in enumerate(results['predictions'][:5]):\n    print(f\"\\n--- Example {i+1} ---\")\n    print(f\"Question: {pred['question']}\")\n    print(f\"Gold SQL: {pred['gold_sql']}\")\n    print(f\"Pred SQL: {pred['pred_sql']}\")\n    print(f\"Correct: {pred['execution_correct']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T20:33:04.841053Z","iopub.execute_input":"2026-02-12T20:33:04.841918Z","iopub.status.idle":"2026-02-12T20:33:04.849202Z","shell.execute_reply.started":"2026-02-12T20:33:04.841878Z","shell.execute_reply":"2026-02-12T20:33:04.848586Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Interactive Testing","metadata":{}},{"cell_type":"code","source":"# Load trained model for interactive testing\nfrom peft import PeftModel\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)\n\n# Load LoRA adapters\ntrained_model = PeftModel.from_pretrained(base_model, \"outputs/rl-text2sql\")\ntrained_model.eval()\n\nprint(\"Trained model loaded!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T20:33:38.889504Z","iopub.execute_input":"2026-02-12T20:33:38.889823Z","iopub.status.idle":"2026-02-12T20:33:42.109662Z","shell.execute_reply.started":"2026-02-12T20:33:38.889795Z","shell.execute_reply":"2026-02-12T20:33:42.108991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Interactive generation\ndef ask_question(question):\n    prompt = f\"Question: {question}\\nSQL:\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(trained_model.device)\n    \n    outputs = trained_model.generate(\n        **inputs,\n        max_new_tokens=256,\n        temperature=0.1,\n        do_sample=False\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract SQL\n    sql = response.split(\"SQL:\")[-1].strip()\n    \n    return sql\n\n# Try some questions\nquestions = [\n    \"What are the names of all students?\",\n    \"How many students are there?\",\n    \"What is the average age of students?\"\n]\n\nfor q in questions:\n    sql = ask_question(q)\n    print(f\"Q: {q}\")\n    print(f\"A: {sql}\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T20:33:50.398541Z","iopub.execute_input":"2026-02-12T20:33:50.399374Z","iopub.status.idle":"2026-02-12T20:34:52.460811Z","shell.execute_reply.started":"2026-02-12T20:33:50.399335Z","shell.execute_reply":"2026-02-12T20:34:52.459982Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Save to Drive (Optional)","metadata":{}},{"cell_type":"code","source":"# Kaggle automatically saves anything in /kaggle/working/ \n# Just copy files there - they'll be available after session ends\n\n!mkdir -p /kaggle/working/outputs\n!cp -r outputs/rl-text2sql /kaggle/working/\n!cp results/predictions.json /kaggle/working/ 2>/dev/null || echo \"No predictions yet\"\n\nprint(\"✅ Model saved to Kaggle outputs (will persist after session)\")\nprint(\"Access via: Notebook → Output → Download\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T20:35:25.240850Z","iopub.execute_input":"2026-02-12T20:35:25.241177Z","iopub.status.idle":"2026-02-12T20:35:25.803544Z","shell.execute_reply.started":"2026-02-12T20:35:25.241144Z","shell.execute_reply":"2026-02-12T20:35:25.802722Z"}},"outputs":[],"execution_count":null}]}