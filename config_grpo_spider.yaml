# Configuration for RL Text-to-SQL Training
# Based on SQL-R1 GRPO approach
# Optimized for Kaggle Notebooks with Unsloth support

# Model Configuration
model_name: "Qwen/Qwen2.5-Coder-3B-Instruct"
use_qlora: true
use_unsloth: false  # Set to true for Kaggle (Linux only, faster training, less VRAM)
use_vllm_inference: false

# Maximum sequence length (important for unsloth)
max_seq_length: 2048

# Hardware Configuration
num_gpus: 1  # Set to 1 for Kaggle (single GPU)

# Data Configuration
train_data: "data/spider/train_spider.json"
dev_data: "data/spider/dev.json"
db_root: "data/spider/database"

# GRPO Configuration
num_samples: 2  # Number of samples per prompt (K in paper)
temperature: 0.8
top_p: 0.9
max_new_tokens: 512

# Reward Configuration
execution_weight: 1.0
partial_weight: 0.3
timeout_seconds: 5
use_partial_rewards: true

# Training Configuration
num_epochs: 2
batch_size: 1  # Keep at 1 for 24GB GPU or Kaggle
gradient_accumulation_steps: 8  # Effective batch size = 8
learning_rate: 1e-5
max_grad_norm: 1.0
warmup_steps: 100

# RL Hyperparameters
kl_coef: 0.1  # KL penalty coefficient (Î²)
clip_range: 0.2  # PPO-style clipping
advantage_normalization: true

# LoRA Configuration (if using PEFT)
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj

# Output
output_dir: "outputs/rl-text2sql"
seed: 42
output_dir: "outputs/rl-text2sql"
log_interval: 10
eval_interval: 100
save_steps: 500

# System
seed: 42
device: "cuda"

# Notes:
# - For 24GB GPU, keep batch_size=1 with gradient accumulation
# - Reduce num_samples to 2-3 if OOM
# - Reduce max_new_tokens to 128 if OOM
# - Use gradient_checkpointing (enabled by default in code)
