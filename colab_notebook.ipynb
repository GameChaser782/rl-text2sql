{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Text-to-SQL Training on Google Colab\n",
    "\n",
    "This notebook demonstrates how to train a Text-to-SQL model with RL on a 24GB GPU (Colab A100).\n",
    "\n",
    "**Runtime:** GPU (A100 recommended)\n",
    "\n",
    "**Estimated time:** 2-4 hours for 3 epochs on Spider train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/yourusername/rl-text2sql.git\n",
    "%cd rl-text2sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -U pip\n",
    "!pip install -q transformers>=4.35.0 accelerate>=0.25.0 peft>=0.7.0 bitsandbytes>=0.41.0\n",
    "!pip install -q trl>=0.7.0 datasets>=2.14.0 pyyaml timeout-decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Spider Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Spider dataset\n",
    "!mkdir -p data\n",
    "!wget -O data/spider.zip https://drive.google.com/uc?export=download&id=1TqleXec_OykOYFREKKtschzY29dUcVAQ\n",
    "!unzip -q data/spider.zip -d data/\n",
    "!ls -la data/spider/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick Test: Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward import SQLRewardCalculator, RewardConfig\n",
    "\n",
    "# Initialize reward calculator\n",
    "config = RewardConfig(\n",
    "    execution_weight=1.0,\n",
    "    partial_weight=0.3,\n",
    "    use_partial_rewards=True\n",
    ")\n",
    "\n",
    "reward_calc = SQLRewardCalculator(db_path=\"dummy.db\", config=config)\n",
    "\n",
    "# Test partial rewards\n",
    "pred_sql = \"SELECT name, age FROM users WHERE age > 18\"\n",
    "gold_sql = \"SELECT name, age FROM users WHERE age > 18 ORDER BY age\"\n",
    "\n",
    "partial = reward_calc.partial_rewards(pred_sql, gold_sql)\n",
    "print(f\"Partial reward: {partial:.3f}\")\n",
    "\n",
    "# Test component extraction\n",
    "components = reward_calc._extract_sql_components(pred_sql)\n",
    "print(f\"\\nSQL Components:\")\n",
    "for comp_type, comp_set in components.items():\n",
    "    print(f\"  {comp_type}: {comp_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model and Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Model name - choose smaller model for faster testing\n",
    "model_name = \"Qwen/Qwen2.5-Coder-3B-Instruct\"  # 3B parameters\n",
    "# Alternative: \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Add LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nModel loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation\n",
    "def generate_sql(question, model, tokenizer):\n",
    "    prompt = f\"Question: {question}\\nSQL:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test\n",
    "question = \"What are the names of all students?\"\n",
    "response = generate_sql(question, model, tokenizer)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Training Data (Small Subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load Spider data\n",
    "with open('data/spider/train_spider.json', 'r') as f:\n",
    "    spider_data = json.load(f)\n",
    "\n",
    "print(f\"Total training examples: {len(spider_data)}\")\n",
    "\n",
    "# For quick testing, use a small subset\n",
    "subset_size = 100  # Adjust based on time constraints\n",
    "train_subset = spider_data[:subset_size]\n",
    "\n",
    "# Save subset\n",
    "with open('data/spider/train_subset.json', 'w') as f:\n",
    "    json.dump(train_subset, f)\n",
    "\n",
    "print(f\"Using {len(train_subset)} examples for training\")\n",
    "print(f\"\\nExample:\")\n",
    "print(json.dumps(train_subset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config for quick training\n",
    "config = {\n",
    "    'model_name': model_name,\n",
    "    'use_qlora': True,\n",
    "    'train_data': 'data/spider/train_subset.json',\n",
    "    'db_root': 'data/spider/database',\n",
    "    'num_samples': 4,\n",
    "    'temperature': 0.7,\n",
    "    'num_epochs': 2,  # Reduced for quick testing\n",
    "    'batch_size': 1,\n",
    "    'gradient_accumulation_steps': 4,  # Reduced for speed\n",
    "    'learning_rate': 1e-5,\n",
    "    'kl_coef': 0.1,\n",
    "    'execution_weight': 1.0,\n",
    "    'partial_weight': 0.3,\n",
    "    'output_dir': 'outputs/rl-model',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Save config\n",
    "import yaml\n",
    "with open('config_colab.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training script\n",
    "!python train_rl.py --config config_colab.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on dev set\n",
    "!python evaluate.py \\\n",
    "    --model_path outputs/rl-model \\\n",
    "    --base_model $model_name \\\n",
    "    --test_data data/spider/dev.json \\\n",
    "    --db_root data/spider/database \\\n",
    "    --output_file results/predictions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display results\n",
    "with open('results/predictions.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Execution Accuracy: {results['metrics']['execution_accuracy']:.2%}\")\n",
    "print(f\"Exact Match:        {results['metrics']['exact_match']:.2%}\")\n",
    "print(f\"Total Examples:     {results['metrics']['total']}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i, pred in enumerate(results['predictions'][:5]):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Question: {pred['question']}\")\n",
    "    print(f\"Gold SQL: {pred['gold_sql']}\")\n",
    "    print(f\"Pred SQL: {pred['pred_sql']}\")\n",
    "    print(f\"Correct: {pred['execution_correct']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model for interactive testing\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load LoRA adapters\n",
    "trained_model = PeftModel.from_pretrained(base_model, \"outputs/rl-model\")\n",
    "trained_model.eval()\n",
    "\n",
    "print(\"Trained model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive generation\n",
    "def ask_question(question):\n",
    "    prompt = f\"Question: {question}\\nSQL:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(trained_model.device)\n",
    "    \n",
    "    outputs = trained_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.1,\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract SQL\n",
    "    sql = response.split(\"SQL:\")[-1].strip()\n",
    "    \n",
    "    return sql\n",
    "\n",
    "# Try some questions\n",
    "questions = [\n",
    "    \"What are the names of all students?\",\n",
    "    \"How many students are there?\",\n",
    "    \"What is the average age of students?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    sql = ask_question(q)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {sql}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save to Drive (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy model to Drive\n",
    "!cp -r outputs/rl-model /content/drive/MyDrive/rl-text2sql-model\n",
    "!cp results/predictions.json /content/drive/MyDrive/\n",
    "\n",
    "print(\"Model and results saved to Google Drive!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
